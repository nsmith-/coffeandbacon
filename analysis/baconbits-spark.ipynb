{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: This is for spark running on parquet files converted from\n",
    "#     baconbits skims. This is a prototype, there is lots of boilerplate.\n",
    "#     We're making it better :-)\n",
    "\n",
    "import pyspark.sql\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# The following line is necessary because we're working in a\n",
    "# virtualenv. Without it, executors will use the wrong interpreter!\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "session = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"baconbits-spark\") \\\n",
    "    .config('spark.executor.memory', \"16g\") \\\n",
    "    .config('spark.executor.cores', \"4\") \\\n",
    "    .config('spark.sql.execution.arrow.enabled',\"true\") \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 500000) \\\n",
    "    .config('spark.driver.maxResultSize',0) \\\n",
    "    .config('spark.dynamicAllocation.minExecutors',37) \\\n",
    "    .config('spark.dynamicAllocation.maxExecutors',250) \\\n",
    "    .config('spark.cores.max',1000) \\\n",
    "    .getOrCreate()\n",
    "sc = session.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "spark = session\n",
    "\n",
    "nparts_per_file = 3\n",
    "thread_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "with open('metadata/samplefiles.json') as f:\n",
    "    temp = json.load(f)\n",
    "    for dsgroup,datasetlist in temp.items():\n",
    "        if dsgroup != 'Hbb_2017': continue\n",
    "        datasets = datasetlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import DoubleType\n",
    "from columns import gghbbcolumns,gghbbcolumns_mc\n",
    "\n",
    "allcolumns = gghbbcolumns + gghbbcolumns_mc\n",
    "\n",
    "skim_root = 'bitsconvert_17042019'\n",
    "\n",
    "def build_union(uniondf,df):\n",
    "    if uniondf is None:\n",
    "        return df\n",
    "    else:\n",
    "        return uniondf.union(df)\n",
    "\n",
    "def read_df(dsloc):\n",
    "    return spark.read.parquet('hdfs:///store/parquet/zprimebits/%s/%s/'%(skim_root,dsloc))\n",
    "\n",
    "def count_df(df):\n",
    "    return df.count()\n",
    "    \n",
    "alldfs = None\n",
    "\n",
    "dfslist = {}\n",
    "with ThreadPoolExecutor(max_workers=thread_workers) as executor:\n",
    "    future_to_ds = {executor.submit(read_df,dataset): dataset for dataset in datasets.keys()}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_ds),\n",
    "                       total=len(future_to_ds),\n",
    "                       desc='loading datasets'):\n",
    "        dataset = future_to_ds[future]       \n",
    "        df = future.result()\n",
    "        df = df.withColumn('dataset', fn.lit(dataset))\n",
    "        isData = False\n",
    "        for mccol in gghbbcolumns_mc:\n",
    "            if mccol not in df.columns:\n",
    "                isdata = True\n",
    "                df = df.withColumn(mccol, fn.lit(0.0))\n",
    "        nfiles = len(os.listdir('/mnt/hdfs/store/parquet/zprimebits/%s/%s/'%(skim_root,dataset)))\n",
    "        df = df.coalesce(nparts_per_file*nfiles)\n",
    "        dfslist[dataset] = df\n",
    "\n",
    "dfcounts = {}\n",
    "with ThreadPoolExecutor(max_workers=thread_workers) as executor:\n",
    "    future_to_ds = {executor.submit(count_df,df): df for ds,df in dfslist.items()}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_ds),\n",
    "                       total=len(future_to_ds),\n",
    "                       desc='counting events'):\n",
    "        dfcounts[future_to_ds[future]] = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('N events to process:',sum(dfcounts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the hbb analysis worker from the cloudpickle file\n",
    "import cloudpickle as cpkl\n",
    "import lz4.frame as lz4f\n",
    "\n",
    "processor_pkl = 'boostedHbbProcessor.cpkl.lz4'\n",
    "processor_instance = None\n",
    "with lz4f.open(processor_pkl, mode=\"rb\") as fin:\n",
    "    processor_instance = cpkl.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lz4.frame as lz4f\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import BinaryType\n",
    "from fnal_column_analysis_tools import processor\n",
    "\n",
    "lz4_clevel = 1\n",
    "\n",
    "@fn.pandas_udf(BinaryType(), fn.PandasUDFType.SCALAR)\n",
    "def compute(dataset,\n",
    "            AK4Puppijet0_dPhi08,AK4Puppijet0_dR08,AK4Puppijet0_deepcsvb,AK4Puppijet0_pt,AK4Puppijet1_dPhi08,\n",
    "            AK4Puppijet1_dR08,AK4Puppijet1_deepcsvb,AK4Puppijet1_pt,AK4Puppijet2_dPhi08,AK4Puppijet2_dR08,\n",
    "            AK4Puppijet2_deepcsvb,AK4Puppijet2_pt,AK4Puppijet3_dPhi08,AK4Puppijet3_dR08,AK4Puppijet3_deepcsvb,\n",
    "            AK4Puppijet3_pt,AK8Puppijet0_N2sdb1,AK8Puppijet0_deepdoubleb,AK8Puppijet0_deepdoublec,\n",
    "            AK8Puppijet0_deepdoublecvb,AK8Puppijet0_eta,AK8Puppijet0_isHadronicV,AK8Puppijet0_isTightVJet,\n",
    "            AK8Puppijet0_msd,AK8Puppijet0_phi,AK8Puppijet0_pt,AK8Puppijet0_pt_JERDown,AK8Puppijet0_pt_JERUp,\n",
    "            AK8Puppijet0_pt_JESDown,AK8Puppijet0_pt_JESUp,AK8Puppijet1_e3_v1_sdb1,AK8Puppijet1_e4_v2_sdb1,\n",
    "            AK8Puppijet1_msd,AK8Puppijet1_phi,AK8Puppijet1_tau32,MetXCorrjerDown,MetXCorrjerUp,MetXCorrjesDown,\n",
    "            MetXCorrjesUp,MetYCorrjerDown,MetYCorrjerUp,MetYCorrjesDown,MetYCorrjesUp,nAK4PuppijetsPt30,\n",
    "            neleLoose,nmuLoose,npu,ntau,passJson,pfmet,pfmetphi,scale1fb,triggerBits,vmuoLoose0_eta,\n",
    "            vmuoLoose0_phi,vmuoLoose0_pt,kfactorEWK,kfactorQCD,genVPhi,genVMass,genVPt):\n",
    "    global processor_instance, lz4_clevel\n",
    "    \n",
    "    \n",
    "        \n",
    "    columns = [AK4Puppijet0_dPhi08,AK4Puppijet0_dR08,AK4Puppijet0_deepcsvb,AK4Puppijet0_pt,AK4Puppijet1_dPhi08,\n",
    "               AK4Puppijet1_dR08,AK4Puppijet1_deepcsvb,AK4Puppijet1_pt,AK4Puppijet2_dPhi08,AK4Puppijet2_dR08,\n",
    "               AK4Puppijet2_deepcsvb,AK4Puppijet2_pt,AK4Puppijet3_dPhi08,AK4Puppijet3_dR08,AK4Puppijet3_deepcsvb,\n",
    "               AK4Puppijet3_pt,AK8Puppijet0_N2sdb1,AK8Puppijet0_deepdoubleb,AK8Puppijet0_deepdoublec,\n",
    "               AK8Puppijet0_deepdoublecvb,AK8Puppijet0_eta,AK8Puppijet0_isHadronicV,AK8Puppijet0_isTightVJet,\n",
    "               AK8Puppijet0_msd,AK8Puppijet0_phi,AK8Puppijet0_pt,AK8Puppijet0_pt_JERDown,AK8Puppijet0_pt_JERUp,\n",
    "               AK8Puppijet0_pt_JESDown,AK8Puppijet0_pt_JESUp,AK8Puppijet1_e3_v1_sdb1,AK8Puppijet1_e4_v2_sdb1,\n",
    "               AK8Puppijet1_msd,AK8Puppijet1_phi,AK8Puppijet1_tau32,MetXCorrjerDown,MetXCorrjerUp,MetXCorrjesDown,\n",
    "               MetXCorrjesUp,MetYCorrjerDown,MetYCorrjerUp,MetYCorrjesDown,MetYCorrjesUp,nAK4PuppijetsPt30,\n",
    "               neleLoose,nmuLoose,npu,ntau,passJson,pfmet,pfmetphi,scale1fb,triggerBits,vmuoLoose0_eta,\n",
    "               vmuoLoose0_phi,vmuoLoose0_pt,kfactorEWK,kfactorQCD,genVPhi,genVMass,genVPt]\n",
    "    \n",
    "    size = AK4Puppijet0_dPhi08.values.size\n",
    "    items = {name:col.values for name,col in zip(allcolumns,columns)}\n",
    "    df = processor.PreloadedDataFrame(size=size, items=items)\n",
    "    df['dataset'] = dataset[0]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    hists = processor_instance.process(df)\n",
    "    \n",
    "    histblob = lz4f.compress(cpkl.dumps(hists),compression_level=lz4_clevel)\n",
    "        \n",
    "    outs = np.full_like(AK8Puppijet0_phi.values,b'',dtype='O')\n",
    "    outs[0] = histblob\n",
    "    \n",
    "    return pd.Series(outs)\n",
    "\n",
    "@fn.pandas_udf(BinaryType(), fn.PandasUDFType.GROUPED_AGG)\n",
    "def agg_histos(df):\n",
    "    global lz4_clevel\n",
    "    goodlines = df[df.str.len() > 0]\n",
    "    outhist = None\n",
    "    for line in goodlines:\n",
    "        temp = cpkl.loads(lz4f.decompress(line))\n",
    "        if outhist is None:\n",
    "            outhist = temp\n",
    "        else:\n",
    "            for key,val in temp.items():\n",
    "                outhist[key] += val\n",
    "    return lz4f.compress(cpkl.dumps(outhist),compression_level=lz4_clevel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheddfs = {ds:df.select(*tuple([\"dataset\"]+allcolumns)).cache() for ds,df in dfslist.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runanalysis(df):    \n",
    "    return df.withColumn('partid', fn.spark_partition_id()) \\\n",
    "             .withColumn('histos', compute(*tuple([\"dataset\"]+allcolumns))) \\\n",
    "             .select('partid','histos') \\\n",
    "             .groupBy('partid').agg(agg_histos('histos')) \\\n",
    "             .groupBy().agg(agg_histos('agg_histos(histos)')) \\\n",
    "             .toPandas()\n",
    "\n",
    "tic = time.time()\n",
    "theresults = {}\n",
    "with ThreadPoolExecutor(max_workers=thread_workers) as executor:\n",
    "    future_to_ds = {executor.submit(runanalysis,df):ds for ds,df in cacheddfs.items()}\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_ds),\n",
    "                       total=len(future_to_ds),\n",
    "                       desc='analysis jobs'):\n",
    "        theresults[future_to_ds[future]] = future.result()\n",
    "dt = time.time() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevt = sum(dfcounts.values())\n",
    "print('total time: ',dt/60)\n",
    "print(\"Î¼s/evt\", dt/nevt*1e6)\n",
    "print(\"Mevt/s\", nevt/dt/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#unpack the returned histogram bytestrings\n",
    "myresults = deepcopy(list(theresults.values()))\n",
    "start = myresults.pop()\n",
    "start = start[start.columns[0]][0]\n",
    "final_accumulator = cpkl.loads(lz4f.decompress(start))\n",
    "for bitstream in myresults:\n",
    "    if bitstream.empty: continue\n",
    "    for key,ahist in cpkl.loads(lz4f.decompress(bitstream[bitstream.columns[0]][0])).items():\n",
    "        final_accumulator[key] += ahist\n",
    "\n",
    "#fill sumw with the normalizations of the full samples\n",
    "normlist = None\n",
    "with lz4f.open('correction_files/sumw_mc.cpkl.lz4','rb') as fin:\n",
    "    normlist = cpkl.load(fin)\n",
    "\n",
    "final_accumulator['sumw'] = deepcopy(normlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnal_column_analysis_tools import hist\n",
    "import gzip\n",
    "import pickle\n",
    "import numexpr\n",
    "\n",
    "processor_instance.postprocess(final_accumulator)\n",
    "\n",
    "nbins = sum(sum(arr.size for arr in h._sumw.values()) for h in final_accumulator.values() if isinstance(h, hist.Hist))\n",
    "nfilled = sum(sum(np.sum(arr>0) for arr in h._sumw.values()) for h in final_accumulator.values() if isinstance(h, hist.Hist))\n",
    "print(\"Processed %.1fM events\" % (nevt/1e6, ))\n",
    "print(\"Filled %.1fM bins\" % (nbins/1e6, ))\n",
    "print(\"Nonzero bins: %.1f%%\" % (100*nfilled/nbins, ))\n",
    "\n",
    "# Pickle is not very fast or memory efficient, will be replaced by something better soon\n",
    "with lz4f.open(\"hists.cpkl.lz4\", mode=\"wb\", compression_level=6) as fout:\n",
    "    cpkl.dump(final_accumulator, fout)\n",
    "\n",
    "#dt = time.time() - tstart\n",
    "#print(\"%.2f us*cpu/event overall\" % (1e6*dt*nworkers/final_accumulators['nentries'], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
