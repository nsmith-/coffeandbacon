{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z peak example\n",
    "This code is designed to run a self-contained simple analysis of CMS data in a columnar format.\n",
    "No part of this should be construed as a finalzed publication-quality analysis: it is incomplete, many of the corrections are simplistic and/or out of date, and no systematic effects have been considered.  It is a technical demonstration only.\n",
    "The data must be downloaded locally, via `download.sh` script in this directory.\n",
    "(A CMS grid certificate will be necessary)\n",
    "\n",
    "The core of the analysis is found in the cell with the heading `#__worker_class__`.\n",
    "The structure of the Worker class is driven by the needs of the 'striped' data delivery system, although here we only use uproot to do data delivery, as encapsulated by the `UprootJob` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from collections import OrderedDict\n",
    "import cloudpickle\n",
    "import zlib\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import uproot\n",
    "\n",
    "import fnal_column_analysis_tools.lumi_tools as lumi_tools\n",
    "import fnal_column_analysis_tools.lookup_tools as lookup_tools\n",
    "import fnal_column_analysis_tools.hist as hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dataset -> files\n",
    "with open(\"metadata/datadef_nano.json\") as fin:\n",
    "    datadef = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab luminosity values and valid data masks\n",
    "# wget https://cms-service-dqm.web.cern.ch/cms-service-dqm/CAF/certification/Collisions17/13TeV/Final/Cert_294927-306462_13TeV_PromptReco_Collisions17_JSON.txt\n",
    "# brilcalc lumi -c /cvmfs/cms.cern.ch/SITECONF/local/JobConfig/site-local-config.xml -b \"STABLE BEAMS\" --normtag=/cvmfs/cms-bril.cern.ch/cms-lumi-pog/Normtags/normtag_PHYSICS.json -u /pb --byls --output-style csv -i Cert_294927-306462_13TeV_PromptReco_Collisions17_JSON.txt > lumi2017.csv\n",
    "lumidata = lumi_tools.LumiData(\"metadata/lumi2017.csv.gz\")\n",
    "lumimask = lumi_tools.LumiMask(\"metadata/Cert_294927-306462_13TeV_PromptReco_Collisions17_JSON.txt\")\n",
    "lumimask_pkl = zlib.compress(cloudpickle.dumps(lumimask))\n",
    "\n",
    "# Import a bunch of correction histograms\n",
    "weightsext = lookup_tools.extractor()\n",
    "correctionDescriptions = open(\"metadata/corrections.txt\").readlines()\n",
    "weightsext.add_weight_sets(correctionDescriptions)\n",
    "weightsext.finalize()\n",
    "# print(list(weightsext._names.keys()))\n",
    "weights_eval = weightsext.make_evaluator()\n",
    "#let's pickle and zip it\n",
    "weval_pickle = zlib.compress(cloudpickle.dumps(weights_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the histograms\n",
    "\n",
    "hists = OrderedDict()\n",
    "\n",
    "dataset = hist.Cat(\"dataset\", \"DAS name\")\n",
    "channel = hist.Cat(\"channel\", \"dilepton flavor\")\n",
    "\n",
    "# underflow = negative weight sum, overflow = positive weight sum\n",
    "hists['genw'] = hist.Hist(\"Events\", dataset, hist.Bin(\"genw\", \"Gen weight\", [0.]))\n",
    "\n",
    "hists['lepton_pt'] = hist.Hist(\"Events\", dataset, channel, \n",
    "                               hist.Bin(\"lep0_pt\", \"Leading lepton $p_{T}$ [GeV]\", 50, 0, 500),\n",
    "                               hist.Bin(\"lep1_pt\", \"Trailing lepton $p_{T}$ [GeV]\", 50, 0, 500),\n",
    "                              )\n",
    "hists['zMass'] = hist.Hist(\"Events\", dataset, channel,\n",
    "                           hist.Bin(\"mass\", \"$m_{\\ell\\ell}$ [GeV]\", 120, 0, 120),\n",
    "                          )\n",
    "\n",
    "hists['profile'] = hist.Hist(\"RowGroups\", \n",
    "                    hist.Cat(\"op\", \"Operation\", sorting='placement'),\n",
    "                    hist.Bin(\"dt\", \"$\\Delta t$ [$\\mu s$]\", 100, 0, 10),\n",
    "                   )\n",
    "\n",
    "hists['trigger'] = hist.Hist(\"Counts\", dataset, hist.Cat(\"trigger\", \"Trigger name\"))\n",
    "\n",
    "# These aren't actually histograms but have the same reducable property with +=\n",
    "hists['lumis_ee'] = lumi_tools.LumiList()\n",
    "hists['lumis_mm'] = lumi_tools.LumiList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines the mapping from dataset to 'process', or element of the stack plot\n",
    "# as well as the cross sections (for later use)\n",
    "process = hist.Cat(\"process\", \"Process\", sorting='placement')\n",
    "process_map = OrderedDict()\n",
    "process_map[\"TTbar\"] = \"TTJets*\"\n",
    "process_map[\"ZJets\"] = \"DYJetsToLL*\"\n",
    "process_map[\"Data\"] = [\"DoubleEG\", \"DoubleMuon\"]\n",
    "\n",
    "import re\n",
    "mc = re.compile(\"^(?!Data)\")\n",
    "data = \"Data\"\n",
    "\n",
    "# xs = {k:v['xs'] for k,v in datadef.items() if v['xs'] > 0.}\n",
    "xs = {\n",
    "    \"DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8\": 18610.,\n",
    "    \"DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8\": 1921.8*3,\n",
    "    \"TTJets_DiLept_TuneCP5_13TeV-madgraphMLM-pythia8\": 87.3,\n",
    "}\n",
    "\n",
    "def processmap(hist):\n",
    "    return hist.group(process, \"dataset\", process_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to extract the pileup distribution in simulation\n",
    "puBins = np.arange(101)\n",
    "\n",
    "mcPu = {}\n",
    "for dataset, info in datadef.items():\n",
    "    if dataset in [\"DoubleEG\", \"DoubleMuon\"]:\n",
    "        continue\n",
    "    flist = info['files']\n",
    "    puHist = np.zeros(puBins.size-1)\n",
    "    for f in flist:\n",
    "        pu = uproot.open(f)['Events']['Pileup_nTrueInt'].array()\n",
    "        np.add.at(puHist, np.clip(np.searchsorted(puBins, pu, side='right'), 0, puHist.size-1), 1.)\n",
    "    mcPu[dataset] = puHist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to extract the pileup distribution in data\n",
    "\n",
    "# https://cms-service-dqm.web.cern.ch/cms-service-dqm/CAF/certification/Collisions17/13TeV/PileUp/pileup_latest.txt\n",
    "with gzip.open(\"metadata/pileup_2017.json.gz\") as fin:\n",
    "    dataPuIn = json.load(fin)\n",
    "\n",
    "dataPu = []\n",
    "for run, info in dataPuIn.items():\n",
    "    for lumiinfo in info:\n",
    "        if lumimask(np.array(int(run)), np.array(lumiinfo[0])):\n",
    "            dataPu.append([int(run)]+lumiinfo)\n",
    "\n",
    "dataPuIn = None\n",
    "dataPu = np.array(dataPu)\n",
    "\n",
    "# Run complete job to get lumilist, then rerun this cell restrict\n",
    "# pileup reweighting to the lumisections corresponding to the data we actually analyze\n",
    "# (which is a tiny portion of Run2017B)\n",
    "if hists['lumis_ee'].array.size > 0:\n",
    "    didx = dataPu[:,:2].astype('u4').view([('run', 'u4'), ('lumi', 'u4')])\n",
    "    rlidx = hists['lumis_ee'].array.astype('u4').view([('run', 'u4'), ('lumi', 'u4')])\n",
    "    dataPu = dataPu[np.isin(didx, rlidx)[:,0]]\n",
    "\n",
    "# Compute data pileup distribution from per-lumisection\n",
    "# instantaneous luminosity distribution (parameterized by mean and width)\n",
    "from scipy.special import erf\n",
    "mBxsec = 69200.\n",
    "intLumi = dataPu[:,2]\n",
    "meanPu = dataPu[:,4]*mBxsec\n",
    "stdPu = dataPu[:,3]*mBxsec\n",
    "dataPuDist = np.sum(\n",
    "    np.diff(\n",
    "        erf(np.subtract.outer(puBins, meanPu)/np.maximum(stdPu,1e-6)/np.sqrt(2)),\n",
    "        axis=0\n",
    "    )*intLumi,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the pileup distributions to check\n",
    "fig, ax = plt.subplots()\n",
    "ax.step(x=puBins, y=np.r_[dataPuDist/dataPuDist.sum(), 0], label='Data PU')\n",
    "mcs = mcPu['DYJetsToLL_M-50_TuneCP5_13TeV-amcatnloFXFX-pythia8']\n",
    "ax.step(x=puBins, y=np.r_[mcs/mcs.sum(), 0], label='MC PU (DYJetsToLL)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correction factor\n",
    "puCorr = {}\n",
    "for k in mcPu:\n",
    "    corr = (dataPuDist / np.maximum(mcPu[k], 1)) / (dataPuDist.sum() / mcPu[k].sum())\n",
    "    puCorr[k] = lookup_tools.dense_lookup.dense_lookup(corr, puBins)\n",
    "puCorr_pkl = zlib.compress(cloudpickle.dumps(puCorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save correction factor (for use in the ROOT version :)\n",
    "import os\n",
    "if os.path.exists(\"correction_files/puReweight0p38fb.root\"):\n",
    "    os.remove(\"correction_files/puReweight0p38fb.root\")\n",
    "fout = uproot.create(\"correction_files/puReweight0p38fb.root\")\n",
    "for k,v in puCorr.items():\n",
    "    h = hist.Hist(\"Ratio\", hist.Bin(\"pu\",\"N pileup\", v._axes))\n",
    "    h._sumw = {(): np.r_[0, v._values, 0, 0]}\n",
    "    fout[k] = hist.export1d(h, axis=\"pu\")\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__worker_class__\n",
    "\n",
    "# This receives the data as numpy arrays attached to the 'events' object passed to Worker.run()\n",
    "# It fills histograms and sends them back to the job collector.  In a distributed system, this\n",
    "# would be a non-trivial operation, but here it is just a facade.\n",
    "\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import copy\n",
    "import zlib\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from fnal_column_analysis_tools.striped import ColumnGroup, PhysicalColumnGroup, jaggedFromColumnGroup\n",
    "from fnal_column_analysis_tools.lumi_tools import LumiList\n",
    "\n",
    "class Worker(object):\n",
    "    def __init__(self):\n",
    "        self.Columns = []\n",
    "        # https://cms-nanoaod-integration.web.cern.ch/integration/master-102X/mc80X_doc.html\n",
    "        self.Columns.extend([\"run\", \"luminosityBlock\", \"LHEWeight_originalXWGTUP\", \"Pileup_nTrueInt\"])\n",
    "        \n",
    "        self.electron_cols = [\"pt\", \"eta\", \"phi\", \"mass\", \"cutBased\", \"pdgId\", \"pfRelIso03_all\"]\n",
    "        self.Columns.extend(\"Electron.\"+col for col in self.electron_cols)\n",
    "\n",
    "        self.muon_cols = [\"pt\", \"eta\", \"phi\", \"mass\", \"tightId\", \"pdgId\", \"pfRelIso04_all\"]\n",
    "        self.Columns.extend(\"Muon.\"+col for col in self.muon_cols)\n",
    "\n",
    "        # Define here dataset priority for overlap removal\n",
    "        self.triggers = OrderedDict()\n",
    "        self.triggers[\"DoubleEG\"] = [\n",
    "                \"HLT_Ele32_WPTight_Gsf_L1DoubleEG\",\n",
    "                \"HLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL_DZ\",\n",
    "            ]\n",
    "        self.triggers[\"DoubleMuon\"] = [\n",
    "                \"HLT_IsoMu24\",\n",
    "                \"HLT_IsoMu27\",\n",
    "                \"HLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL_DZ\",\n",
    "            ]\n",
    "        for triggers in self.triggers.values(): self.Columns.extend(triggers)\n",
    "        \n",
    "        self.weights_eval = None\n",
    "        self.lumimask = None\n",
    "        self.puCorr = None\n",
    "        \n",
    "    def run(self, events, job):\n",
    "        times = OrderedDict()\n",
    "        times['start'] = time.time()\n",
    "        \n",
    "        if self.weights_eval is None:\n",
    "            self.weights_eval = cloudpickle.loads(zlib.decompress(job[\"weights_eval\"]))\n",
    "        if self.lumimask is None:\n",
    "            self.lumimask = cloudpickle.loads(zlib.decompress(job[\"lumimask\"]))\n",
    "        if self.puCorr is None:\n",
    "            self.puCorr = cloudpickle.loads(zlib.decompress(job[\"puCorr\"]))\n",
    "\n",
    "\n",
    "        dataset = job['dataset']\n",
    "        hists = job['hists']\n",
    "\n",
    "        isRealData = False\n",
    "        good_trigger = np.zeros(events.nevents, dtype=bool)\n",
    "        if dataset in self.triggers:\n",
    "            isRealData = True\n",
    "            # Real data, prevent overlaps\n",
    "            overlap_veto = np.zeros_like(good_trigger)\n",
    "            for ds, tlist in self.triggers.items():\n",
    "                if ds != dataset:\n",
    "                    for trigger in tlist:\n",
    "                        overlap_veto |= getattr(events, trigger)\n",
    "                        hists['trigger'].fill(dataset=dataset, trigger=trigger, weight=getattr(events, trigger).sum())\n",
    "                else:\n",
    "                    for trigger in tlist:\n",
    "                        good_trigger |= getattr(events, trigger)\n",
    "                        hists['trigger'].fill(dataset=dataset, trigger=trigger, weight=getattr(events, trigger).sum())\n",
    "                    break\n",
    "            good_trigger &= ~overlap_veto\n",
    "            # apply 'golden json' certified lumiblocks\n",
    "            good_trigger &= lumimask(events.run, events.luminosityBlock)\n",
    "            # record lumis processed\n",
    "            #  in uproot jobs, full lumi completion guaranteed by full-file processing\n",
    "            #  in striped jobs, partial lumi completion is not traceable at present\n",
    "            if dataset == \"DoubleEG\":\n",
    "                hists['lumis_ee'] += LumiList(events.run, events.luminosityBlock)\n",
    "            elif dataset == \"DoubleMuon\":\n",
    "                hists['lumis_mm'] += LumiList(events.run, events.luminosityBlock)\n",
    "        else:\n",
    "            # MC, OR all\n",
    "            for tlist in self.triggers.values():\n",
    "                for trigger in tlist:\n",
    "                    good_trigger |= getattr(events, trigger)\n",
    "                    hists['trigger'].fill(dataset=dataset, trigger=trigger, weight=getattr(events, trigger).sum())\n",
    "            genw = np.sign(events.LHEWeight_originalXWGTUP)\n",
    "            puw = self.puCorr[dataset](events.Pileup_nTrueInt)\n",
    "        \n",
    "        times['lumimask, trigger'] = time.time()\n",
    "       \n",
    "        electronCols = PhysicalColumnGroup(events, \"Electron\", **{k:k for k in self.electron_cols})\n",
    "        electrons = jaggedFromColumnGroup(electronCols)  \n",
    "        electrons['weight'] = weights_eval[\"eleScaleFactor_TightId_POG\"](electrons.p4.eta, electrons.p4.pt)\n",
    "\n",
    "        muonCols = PhysicalColumnGroup(events, \"Muon\", **{k:k for k in self.muon_cols})\n",
    "        muons = jaggedFromColumnGroup(muonCols)\n",
    "        muons['weight'] = weights_eval[\"muScaleFactor_TightId_Iso\"](np.abs(muons.p4.eta), muons.p4.pt)\n",
    "                \n",
    "        ele = electrons[(electrons.p4.pt > 20) &\n",
    "                                           (np.abs(electrons.p4.eta) < 2.5) &\n",
    "                                           (electrons.cutBased >= 4)]\n",
    "        \n",
    "        mu = muons[(muons.p4.pt > 20) &\n",
    "                                   (np.abs(muons.p4.eta) < 2.4) &\n",
    "                                   (muons.tightId > 0)]\n",
    "        \n",
    "        times['good leptons'] = time.time()\n",
    "\n",
    "        ee = ele.distincts()\n",
    "        mm = mu.distincts()\n",
    "        em = ele.cross(mu)\n",
    "        \n",
    "        dileptons = {}\n",
    "        dileptons['ee'] = ee[(ee.i0.pdgId*ee.i1.pdgId == -11*11) & (ee.i0.p4.pt > 25)]\n",
    "        dileptons['mm'] = mm[(mm.i0.pdgId*mm.i1.pdgId == -13*13)]\n",
    "        dileptons['em'] = em[(em.i0.pdgId*em.i1.pdgId == -11*13)]\n",
    "        \n",
    "        times['good pairs'] = time.time()\n",
    "        \n",
    "        channels = {}\n",
    "        channels['ee'] = good_trigger & (ee.counts == 1) & (mu.counts == 0)\n",
    "        channels['mm'] = good_trigger & (mm.counts == 1) & (ele.counts == 0)\n",
    "        channels['em'] = good_trigger & (em.counts == 1) & (ele.counts == 1) & (mu.counts == 1)\n",
    "        \n",
    "        times['channels'] = time.time()\n",
    "        \n",
    "        if not isRealData:\n",
    "            hists['genw'].fill(dataset=dataset, genw=genw)\n",
    "\n",
    "        dupe = np.zeros(events.nevents, dtype=bool)\n",
    "        tot = 0\n",
    "        for channel, cut in channels.items():\n",
    "            zcands = dileptons[channel][cut]\n",
    "            dupe |= cut\n",
    "            tot += cut.sum()\n",
    "            weight = np.array(1.)\n",
    "            if not isRealData:\n",
    "                weight = zcands.i0['weight'] * zcands.i1['weight'] * genw[cut] * puw[cut]\n",
    "            hists['zMass'].fill(dataset=dataset, channel=channel,\n",
    "                                mass=zcands.p4.mass.flatten(),\n",
    "                                weight=weight.flatten(),\n",
    "                               )\n",
    "            # only fill for m > 50\n",
    "            weight = weight.flatten() * (zcands.p4.mass > 50.).flatten()\n",
    "            hists['lepton_pt'].fill(dataset=dataset, channel=channel, \n",
    "                                    lep0_pt=zcands.i0.pt.flatten(),\n",
    "                                    lep1_pt=zcands.i0.pt.flatten(),\n",
    "                                    weight=weight,\n",
    "                                   )\n",
    "      \n",
    "        if dupe.sum() != tot:\n",
    "            raise Exception(\"Double-counting events!\")\n",
    "            \n",
    "        times['plots'] = time.time()\n",
    "        \n",
    "        #profiling info\n",
    "        t = list(times.values())\n",
    "        for i, name in enumerate(times):\n",
    "            if i==0: continue\n",
    "            dt = t[i] - t[i-1]\n",
    "            hists['profile'].fill(op=name, dt=1e6*(dt/len(events.Muon.count)))\n",
    "        hists['profile'].fill(op=\"total\", dt=1e6*(t[-1]-t[0])/events.nevents)\n",
    "        \n",
    "        job.send(hists=hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This sets up and displays real-time-updateable set of plots\n",
    "from fnal_column_analysis_tools.hist import plot\n",
    "from fnal_column_analysis_tools.striped import HistCollectorCallback\n",
    "\n",
    "fill_opts = {'edgecolor': (0,0,0,0.3), 'alpha': 0.8}\n",
    "error_opts = {'label':'Stat. Unc.', 'hatch':'///', 'facecolor':'none', 'edgecolor':(0,0,0,.5), 'linewidth': 0}\n",
    "data_error_opts = {'linestyle':'none', 'marker': '.', 'markersize': 10., 'color':'k', 'elinewidth': 1, 'emarker': '_', 'drawstyle': 'default'}\n",
    "\n",
    "class PlotDrawer(object):    \n",
    "    def __call__(self):\n",
    "        figs = []\n",
    "        figs.append(self.drawmass_ee())\n",
    "        figs.append(self.drawprofile())\n",
    "        return figs\n",
    "    \n",
    "    def drawmass_ee(self):\n",
    "        figin = getattr(self, 'massfig', None)\n",
    "        lumi = lumidata.get_lumi(hists['lumis_mm'])\n",
    "        scale = {k[0]: lumi*xs[k[0]]/(v[1]-v[0]) for k,v in hists['genw'].values(overflow='all').items() if k[0] in xs}\n",
    "        hscaled = hists['zMass'].project(\"channel\", \"mm\")\n",
    "        hscaled.scale(scale, axis=\"dataset\")\n",
    "        hproc = processmap(hscaled)\n",
    "        fig, ax, _ = plot.plot1d(hproc[mc], overlay='process', ax=figin, stack=True, fill_opts=fill_opts, error_opts=error_opts)\n",
    "        plot.plot1d(hproc[data], overlay='process', ax=ax, clear=False, error_opts=data_error_opts)\n",
    "        ax.text(1., 1., r\"%.2f fb$^{-1}$ (13 TeV)\" % (lumi*1e-3), fontsize=16, horizontalalignment='right', verticalalignment='bottom', transform=ax.transAxes)\n",
    "        ax.set_yscale('log')\n",
    "        ax.autoscale(axis='y', tight=False)\n",
    "        ax.set_ylim(1, None)\n",
    "        self.massfig = ax\n",
    "        return fig\n",
    "\n",
    "    def drawprofile(self):\n",
    "        axin = getattr(self, 'profileaxis', None)\n",
    "        fig, axis, _ = plot.plot1d(hists['profile'], overlay='op', ax=axin, line_opts={})\n",
    "        axis.legend(title=hists['profile'].axis('op').label)\n",
    "        self.profileaxis = axis\n",
    "        return fig\n",
    "\n",
    "\n",
    "draw = PlotDrawer()\n",
    "callback = HistCollectorCallback(hists, draw, update=1e6)\n",
    "# This cell must stay separated from the update loop\n",
    "# https://github.com/matplotlib/jupyter-matplotlib/issues/17\n",
    "figs = draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This runs the worker over the data we defined earlier\n",
    "from fnal_column_analysis_tools.striped import UprootJob\n",
    "\n",
    "for h in hists.values(): h.clear()\n",
    "    \n",
    "from pyinstrument import Profiler\n",
    "profiler = Profiler()\n",
    "\n",
    "t0 = time.time()\n",
    "profiler.start()\n",
    "\n",
    "threads = {}\n",
    "events = 0\n",
    "for dataset, info in datadef.items():\n",
    "    flist = info['files']\n",
    "    job = UprootJob(\n",
    "        dataset=dataset,\n",
    "        filelist=flist,\n",
    "        treename=\"Events\",\n",
    "        worker_class=Worker,\n",
    "        user_callback=callback,\n",
    "        user_params={\n",
    "                \"weights_eval\": weval_pickle,\n",
    "                \"lumimask\": lumimask_pkl,\n",
    "                \"puCorr\": puCorr_pkl,\n",
    "                \"dataset\": dataset,\n",
    "                \"hists\": hists,\n",
    "            },\n",
    "    )\n",
    "    threads[dataset] = job.run(workers=0, progress=False, threaded=False)\n",
    "    events += job.EventsProcessed\n",
    "\n",
    "t1 = time.time()\n",
    "profiler.stop()\n",
    "print(profiler.output_text(unicode=True, color=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t1-t0)*1e6/events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figin = None\n",
    "lumi = lumidata.get_lumi(hists['lumis_mm'])\n",
    "scale = {k[0]: lumi*xs[k[0]]/(v[1]-v[0]) for k,v in hists['genw'].values(overflow='all').items() if k[0] in xs}\n",
    "hscaled = hists['lepton_pt'].project(\"channel\", \"mm\").sum('lep1_pt')\n",
    "hscaled.scale(scale, axis=\"dataset\")\n",
    "hproc = processmap(hscaled)\n",
    "fig, ax, _ = plot.plot1d(hproc[mc], overlay='process', ax=figin, stack=True, fill_opts=fill_opts, error_opts=error_opts)\n",
    "plot.plot1d(hproc[data], overlay='process', ax=ax, clear=False, error_opts=data_error_opts)\n",
    "ax.text(1., 1., r\"%.2f fb$^{-1}$ (13 TeV)\" % (lumi*1e-3), fontsize=16, horizontalalignment='right', verticalalignment='bottom', transform=ax.transAxes)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1, 1e7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists['genw'].values(overflow='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"profile.html\", \"w\") as fout:\n",
    "    fout.write(profiler.output_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "with gzip.open(\"hists.pkl.gz\", \"wb\") as fout:\n",
    "    pickle.dump(hists, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is currently not functional\n",
    "\n",
    "from striped.job import SinglePointStripedSession as Session\n",
    "job_server = (\"ifdb02.fnal.gov\", 8766)\n",
    "session = Session(job_server)\n",
    "\n",
    "nevents_total = 0\n",
    "t1 = time.time()\n",
    "for dataset in datasets:\n",
    "    job = session.createJob(dataset, \n",
    "            fraction=0.1,\n",
    "            user_callback=callback,\n",
    "            user_params = {\n",
    "                \"weights_eval\": weval_pickle,\n",
    "                \"dataset\": dataset,\n",
    "                \"hists\": hists,\n",
    "                \"profile\": profile,\n",
    "            },\n",
    "    )\n",
    "    job.run()\n",
    "    runtime = job.TFinish - job.TStart\n",
    "    nevents = job.EventsProcessed\n",
    "    nevents_total += nevents\n",
    "    print(\"%-70s %7.3f M events, %7.3f M events/sec\" % (dataset[:70], float(nevents)/1e6, nevents/runtime/1000000))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Total events processed: %d in %.1f seconds -> %.6f million events/second\" %(nevents_total, t2-t1, nevents_total/(t2-t1)/1000000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
